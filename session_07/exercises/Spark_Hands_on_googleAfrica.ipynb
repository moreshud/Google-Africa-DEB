{"cells":[{"cell_type":"markdown","metadata":{"id":"Qgaeck94a1jM"},"source":["# Install Spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQq08nU6RtS_"},"outputs":[],"source":["#If you are using  Data proc comment the file for spark instalattion.\n","# !sudo apt update\n","# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# #Update spark version for install\n","# !wget -q  https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n","# !tar xf spark-3.3.2-bin-hadoop3.tgz\n","\n","# import os\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n","\n","\n","# !pip install -q findspark\n","# !pip install pyspark\n","\n","# import findspark\n","# findspark.init()\n","# findspark.find()\n","\n","from pyspark.sql import DataFrame, SparkSession\n","\n","\n","\n","spark = SparkSession \\\n","       .builder \\\n","       .appName(\"Our First Spark example\") \\\n","       .getOrCreate()\n","\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkVW8KPLqOVJ"},"outputs":[],"source":["spark = SparkSession \\\n","       .builder \\\n","       .appName(\"Our First Spark example\") \\\n","       .getOrCreate()\n","spark"]},{"cell_type":"markdown","metadata":{"id":"At82KKPRbITu"},"source":["# Download the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-I-BMFYWOvG","outputId":"af0614ee-2067-41e9-ebbd-236dc1319634"},"outputs":[],"source":["import urllib\n","url = 'https://drive.google.com/file/d/1BFx9CClKTe_syBuIr5WBi5uuSkZf9wdv/view?usp=sharing'\n","url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n","urllib.request.urlretrieve(url, \"people-with-dups.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"gQ_zkCenl3mk"},"source":["# Instructions\n","\n","##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) De-Duping Data Lab\n","\n","In this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n","\n","* first, middle and last names\n","* gender\n","* birth date\n","* Social Security number\n","* salary\n","\n","But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n","\n","* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n","* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n","\n","The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. The salaries will match as well,\n","and the Social Security Numbers *would* match if they were somehow put in the same format.\n","\n","Your job is to remove the duplicate records. The specific requirements of your job are:\n","\n","* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n","* Standarize the field ssn with out hyphens.\n","* Create a colum age base on the birth date.\n","\n","\n","Finally, you will write the results as a Parquet file.\n","\n","\n","**Hint:** The initial dataset contains 10,000 records.<br/>\n","The de-duplicated result has 9,976 records.\n"]},{"cell_type":"markdown","metadata":{"id":"FvpORm7WmO-9"},"source":["# Read the file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#load the file into HDFS system\n","# Run this just is are using a data proc cluster\n","!hdfs dfs -put people-with-dups.csv /user/root/people-with-dups.csv\n","!hdfs dfs -ls /user/root/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X39uPH1OR8Bi"},"outputs":[],"source":["#Read data from csv file\n","#Optional read the csv file from a gcp bucket\n","from pyspark.sql.functions import *\n","path = \"people-with-dups.csv\"\n","\n","df = (spark\n","    .read\n","    .option(\"header\", \"true\")\n","    .option(\"inferSchema\", \"true\")\n","    .csv(path))\n","\n","#see just 10 record of the data set\n","df.show(10)\n"," "]},{"cell_type":"markdown","metadata":{"id":"V9iS3_Muux7v"},"source":[]},{"cell_type":"markdown","metadata":{"id":"UPA4ImypmRQh"},"source":["# Processing\n","You can use the [SQL API reference](https://hyukjin-spark.readthedocs.io/en/latest/reference/pyspark.sql.html) for help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5iiyYIRmXVs"},"outputs":[],"source":["#Create new name columns with only lower case names\n","dedupedDF = df.withColumn('firstName_lower', lower(col('firstName'))) \\\n","          .withColumn('middleName_lower', lower(col('middleName'))) \\\n","          .withColumn('lastName_lower', lower(col('lastName')))\n","\n","#Create new ssn column without any dashes\n","\n","#Create a new age field\n","\n","#Drop duplicates based only on the new columns, gender, birthDate and salary\n","\n","#Drop the old columns\n","\n","\n","\n","#See everything is ok with the final df\n","dedupedDF.show(5)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvZT_GBl2l63","outputId":"a0384368-0c8c-4b66-8ba1-2b683b157b8c"},"outputs":[],"source":["#See the number of rows\n"]},{"cell_type":"markdown","metadata":{"id":"CUwfDVbjmUD4"},"source":["# Save the file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pa-VZ1JQnYTs"},"outputs":[],"source":["#Write the processed data frame on parquet format\n","#Optional Write the csv file in a gcp bucket\n","path = \"people-deduped.parquet\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["#validate if the parque file was writed in the HDFS\n","!hdfs dfs -ls /user/root/people_without_dups"]},{"cell_type":"markdown","metadata":{},"source":["# Analytics process"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instructions\n","Add the code in Spark to do the below requirements.\n","How much the process of deduplication affect the Average Salary (do a difference before and after cleaning the data)\n","\n","There is suspicion this data was simulated, What is the Average Salary grouping in ages with 10-year intervals (0-10, 10-20, 20-30), just for people from 20 to 60 years old. \n","\n","Finally, you will write the results as a Parquet file."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the average Salary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Get Average Salary grouping"]},{"cell_type":"markdown","metadata":{"id":"JmV_DDyT2pUU"},"source":["# Check your answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV0AM1Z-2p9s"},"outputs":[],"source":["finalCount = dedupedDF.count()\n","assert finalCount == 9976, \"expected 9976 records in finalDF\""]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}